scala


sundog-education.com/sparkscala

windows:
	1. 进入项目路径：
		cd E:\scala\spark-box
	2. 编译：
		sbt package

	3. 提交执行任务：
		spark-submit --class WZLX0X12Y2Y3Handler --master local[*] --packages "org.apache.spark:spark-sql-kafka-0-10_2.12:3.0.1,org.mongodb.spark:mongo-spark-connector_2.12:3.0.0" target/scala-2.12/simple-project_2.12-1.0.jar
		
		

sentimen项目：
	1. 进入项目路径：
		cd E:\scala\scala-demo
	2. 编译：
		sbt package

	3. 提交执行任务：
		spark-submit --class DataAssembleWithLogFailerAtlas --master local[*] --jars /home/zy/wzl/jars/mysql-connector-java-8.0.22.jar target/scala-2.11/simple-project_2.11-1.0.jar /home/zy/wzl/spark-box/apis/dataIntegration.json
		
		

		
		
		spark-submit --class mongoConnector --master local[*] --packages "org.mongodb.spark:mongo-spark-connector_2.11:2.4.2" target/scala-2.11/simple-project_2.11-1.0.jar
		
		
		spark-submit --class mongoReader --master local[*] --packages "org.mongodb.spark:mongo-spark-connector_2.11:2.4.2" target/scala-2.11/simple-project_2.11-1.0.jar
		
		# 
		spark-submit --class LivyBatchDemo --master yarn --packages "org.mongodb.spark:mongo-spark-connector_2.11:2.4.2" target/scala-2.11/simple-project_2.11-1.0.jar
		
		# 执行JsonParamsDemo
		spark-submit --class JDBCConnectors --master local[6] target/scala-2.11/simple-project_2.11-1.0.jar 
		
		
		spark-submit --class InfluxDBConnector --master local[6] --packages "com.influxdb:influxdb-client-java:1.14.0,com.influxdb:influxdb-client-core:1.14.0,com.squareup.retrofit2:retrofit:2.9.0" target/scala-2.11/simple-project_2.11-1.0.jar 
		
		spark-submit --class PipelineDemo --master local[6]  --packages org.apache.spark:spark-hive_2.11:2.4.7 target/scala-2.11/simple-project_2.11-1.0.jar
		
		
		spark-submit --class InfluxdbDemo --master local[*] InfluxdbDemo --jars /usr/local/hadoop/hadoop-2.7.0/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar --packages target/scala-2.11/simple-project_2.11-1.0.jar
		
		spark-submit --class InfluxdbDemo --master local[*] --packages "com.influxdb:influxdb-client-core:1.5.0,com.influxdb:influxdb-client-java:1.5.0" target/scala-2.11/simple-project_2.11-1.0.jar
		
		
		spark-submit --class InfluxdbDemo --master local[*] --packages com.influxdb:influxdb-client-core:1.14.0,com.influxdb:influxdb-client-java:1.14.0,org.jetbrains.kotlin:kotlin-stdlib:1.3.71,com.google.code.gson:gson:2.8.5 --jars /home/zy/.ivy2/jars/com.google.code.gson_gson-2.8.5.jar --conf spark.driver.extraClassPath=/home/zy/.ivy2/jars/com.google.code.gson_gson-2.8.5.jar target/scala-2.11/simple-project_2.11-1.0.jar
		
		# 本地运行
		docker exec -it spark-master /spark/bin/spark-submit --class LivyBatchDemo --master local[*] target/scala-2.11/simple-project_2.11-1.0.jar
		
		# standalone （集群）
		docker exec -it spark-master /spark/bin/spark-submit --class LivyBatchDemo --master spark://spark-master:7077 target/scala-2.11/simple-project_2.11-1.0.jar
		
		
		
scala中的json库：
	https://www.cnblogs.com/ShyPeanut/p/12612678.html
		
		


val spark = SparkSession.builder.appName("dataConnector").config("spark.hadoop.hive.metastore.warehouse.dir", "/home/zy/tools/apache-hive-2.2.0-bin/metastore_db/").enableHiveSupport().getOrCreate()


在IDEA中使用SBT构建工具创建SCALA项目
	https://blog.csdn.net/leo3070/article/details/80040400
	
	

集成atlas：
spark-submit --class DataAssembleWithLogFailerAtlas \
--master local[*] --jars /home/zy/wzl/jars/mysql-connector-java-8.0.22.jar,/home/zy/wzl/jars/spark-atlas-connector-assembly-0.1.0-SNAPSHOT.jar,/home/zy/wzl/jars/spark-hive_2.11-2.4.7.jar,/home/zy/wzl/jars/hive-common-2.3.8.jar,/home/zy/wzl/jars/spark-hive-thriftserver_2.11-2.4.7.jar \
--files /home/zy/ws/apache-atlas-1.2.0/conf/atlas-application.properties \
--conf spark.extraListeners=com.hortonworks.spark.atlas.SparkAtlasEventTracker \
--conf spark.sql.queryExecutionListeners=com.hortonworks.spark.atlas.SparkAtlasEventTracker \
--conf spark.sql.streaming.streamingQueryListeners=com.hortonworks.spark.atlas.SparkAtlasStreamingQueryEventTracker \
target/scala-2.11/simple-project_2.11-1.0.jar /home/zy/wzl/spark-box/apis/dataIntegration.json


嘉净历史数据进hive：
	spark-submit --class JiaJingMonitorToHive --master local[*] --packages "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.6,org.apache.kafka:kafka-clients:1.1.0,org.apache.spark:spark-hive_2.11:2.4.7,org.apache.spark:spark-hive-thriftserver_2.11:2.4.7" target/scala-2.11/simple-project_2.11-1.0.jar /home/zy/wzl/spark-box/JiajingConfig/conf.jiajing.history.json
	
	spark-submit --class SparkHive --master local[6] --packages "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.6,org.apache.kafka:kafka-clients:1.1.0,org.apache.spark:spark-hive_2.11:2.4.7,org.apache.spark:spark-hive-thriftserver_2.11:2.4.7" target/scala-2.11/simple-project_2.11-1.0.jar /home/zy/wzl/spark-box/apis/conf.jiajing.history.json
	
	# 常规巡检
	spark-submit --class Insp --master local[*] --packages "com.redislabs:spark-redis_2.11:2.4.2,org.mongodb.spark:mongo-spark-connector_2.11:2.4.2,com.rabbitmq:amqp-client:4.11.0" target/scala-2.11/simple-project_2.11-1.0.jar  /home/zy/wzl/spark-box/JiajingConfig/insp.json
	
	
	# spark-shell 操作hive
	spark-shell --packages org.apache.spark:spark-hive_2.11:2.4.7 --conf hive.metastore.uris=thrift://localhost:9083
	
	# 阶段巡检
	spark-submit --class InspDuration --master local[6] --packages "org.apache.spark:spark-hive_2.11:2.4.7,org.apache.spark:spark-hive-thriftserver_2.11:2.4.7" target/scala-2.11/simple-project_2.11-1.0.jar
	

	

scala 获取时间，日期，前天，昨天，上一个小时等等：
	https://blog.csdn.net/weixin_41804049/article/details/85002373
	
	
自定义隐式转换，将bool值转为数值：https://stackoverflow.com/questions/2633719/is-there-an-easy-way-to-convert-a-boolean-to-an-integer
	If you want to mix Boolean and Int operation use an implicit as above but without creating a class:

	implicit def bool2int(b:Boolean) = if (b) 1 else 0

	scala> false:Int
	res4: Int = 0

	scala> true:Int
	res5: Int = 1

	scala> val b=true
	b: Boolean = true


	scala> 2*b+1
	res2: Int = 3