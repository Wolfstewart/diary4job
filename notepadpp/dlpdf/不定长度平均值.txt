# 4. Build Loss， 不定长度的损失，都可用这种方法计算，动态平均值
            with tf.variable_scope("loss"):
                # cross:计算的是每个序列的每个时刻的交叉熵损失值，形状为: [N,T]
                cross = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.output, logits=self.scores)
                with tf.variable_scope("loss_concat"):
                    cross_shape = tf.shape(cross)
                    time_steps = cross_shape[1]  # 序列长度
                    time = tf.constant(0, dtype=tf.int32, name="time")  # 迭代的步长状态值
                    zero_output = tf.zeros_like(cross)[:, 0]  # [N,]
                    outputs = tf.zeros_like(zero_output)  # [N,]

                    def _time_output(time, outputs):
                        # output: [B, U]
                        output = tf.where(
                            condition=time < self.lengths,  # 判断条件，当判断条件为True的时候，返回x的值，否则返回y的值
                            x=cross[:, time],  # 当time处于序列实际长度范围之内的时候，返回损失函数的对应输出值
                            y=zero_output  # 当time不处于序列实际长度方位之内的时候，返回0
                        )
                        # 拼接
                        outputs = outputs + tf.multiply(output, 1.0 / tf.cast(self.lengths, tf.float32))
                        return time + 1, outputs

                    _, output = tf.while_loop(
                        cond=lambda time, _: time < time_steps,
                        body=_time_output,
                        loop_vars=(time, outputs)
                    )
                self.losses = tf.reduce_mean(output)