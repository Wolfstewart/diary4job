内容：
	 梯度下降法、拉格朗日乘子法、KKT条件回顾 
	 感知器模型回顾
	 SVM线性可分
	 SVM线性不可分
	 核函数
	 SMO
最优化问题一般是指对于某一个函数而言，求解在其指定作用域上的全局最小值
问题，一般分为以下三种情况(备注：以下几种方式求出来的解都有可能是局部
极小值，只有当函数是凸函数的时候，才可以得到全局最小值)：
	 无约束问题：求解方式一般求解方式梯度下降法、牛顿法、坐标轴下降法等；
	 等式约束条件：求解方式一般为拉格朗日乘子法
	 不等式约束条件：求解方式一般为KKT条件

梯度下降法(Gradient Descent， GD)
常用于求解无约束情况下凸函数(Convex Function)的极小值，是一种迭代类型的
算法，因为凸函数只有一个极值点，故求解出来的极小值点就是函数的最小值点。


感知器模型：
	 感知器算法是最古老的分类算法之一，原理比较简单，不过模型的分类泛化能力比较弱，不过感知器模型是SVM、神经网络、深度学习等算法的基础
	感知器模型的前提是：数据是线性可分的。
	
SVM：分类多用SVM，回归多用集成学习的Boosting

	支持向量机(Support Vecor Machine, SVM)本身是一个二元分类算法，
	是对感知器算法模型的一种扩展，现在的SVM算法支持线性分类和非线性
	分类的分类应用，并且也能够直接将SVM应用于回归应用中，同时通过OvR
	或者OvO的方式我们也可以将SVM应用在多元分类领域中。在不考虑集成学
	习算法，不考虑特定的数据集的时候，在分类算法中SVM可以说是特别优秀
	的。
	
	线性可分SVM：线性可分时效果比逻辑回归好。
	 1. 要求数据必须是线性可分的；
	 2. 纯线性可分的SVM模型对于异常数据的预测可能会不太准；
	 3. 对于线性可分的数据，线性SVM分类器的效果非常不错。

软间隔（线性可分有异常点）：
	SVM对于训练集中的每个样本都引入一个松弛因子(ξ)，使得函数距离加上松
	弛因子后的值是大于等于1；这表示相对于硬间隔，对样本到超平面距离的
	要求放松了。
	

非线性可分：完全不可以线性可分的数据	
	将数据映射到高维空间中，那么数据就会变成线性可分的，从而就可以使用线性可分SVM模型或者软间隔线性可分SVM模型
	
	直接映射会大大增加计算量：
	原始空间是n维，二阶扩展后我们会得到一个n(n+3)/2维的新空间；
	这个数目是呈爆炸性增长的，这给计算带来了非常大的困难
	
	核函数：在低维空间上的计算量等价于特征做维度扩展后的点乘的结果。

	核函数可以自定义；核函数必须是正定核函数，即Gram矩阵是半正定矩阵

不同核函数效果比较：
	耗时：多项式耗最多，线性最少。
	

不同惩罚项系数比较：
	效果（训练准确率和测试准确率之间的相差小）随着C先增大后减少，即从差到优再到过拟合的过程，C越大，要求训练数据分的越准确，越可能出现过拟合。达到一定条件后随着C的增大耗时越来越大。

SVR:svm回归算法。
	SVM和决策树一样，可以将模型直接应用到回归问题中
	在SVR中，目的是为了尽量拟合一个线性模型y=wx+b；从而我们可以定义常量eps>0，对于任意一点(x,y)，如果|y-wx-b|≤eps，那么认为没有损失


    C越大，表示模型越不允许样本分错，模型在训练数据上的效果就不错，但是在测试数据上可能效果会不佳
    C越小，表示模型的泛化能力越强(中间的间隔越大)
    当核函数为rbf，gamma值越大，模型越容易过拟合，gamma值越小，模型越容易欠拟合(模型的泛化能力就会越强)





