随堂笔记：
集成学习：（思想：三个臭皮匠顶一个诸葛亮）
集成学习的成功在于，保证了弱分类器的多样性。
常见的集成学习思想有：Bagging,Boosting,Stacking(不常用)
为什么要集成学习：
 1. 弱分类器间存在一定的差异性，这会导致分类的边界不同，也就是说可能
 存在错误。那么将多个弱分类器合并后，就可以得到更加合理的边界，减少
 整体的错误率，实现更好的效果；
 2. 对于数据集过大或者过小，可以分别进行划分和有放回的操作产生不同的
 数据子集，然后使用数据子集训练不同的分类器，最终再合并成为一个大的分
 类器；（记死）
 3. 如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训
 练多个模型，然后再进行模型的融合；
 4. 对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集
 构建一个分类模型，然后将多个模型融合。

一、 Bagging：并行，样本不一样，解决过拟合问题。
	内部算法必须一样，否则不是Bagging。
	使用多数投票或者求均值的方式来统计最终的分类/回归结果
	Bagging方法的弱学习器可以是基本的算法模型，
	eg: Linear、Ridge、Lasso、Logistic、Softmax、
	ID3、C4.5、CART、SVM、KNN等。较多的是用于决策树(CART)
  1.随机森林：应用广泛，分类、回归都有，特征转换、异常点检测，解决过拟合
	   1. 随机森林，两个随机，样本随机、特征随机（特征局部最优）。
	   2. 高方差，低偏差，就是过拟合。低方差，高偏差，就是欠拟合
	   3. 随机森林可以减缓过拟合问题。
	代码：algo = RandomForestClassifier(n_estimators=10,max_depth=3,oob_score=True,random_state=28)
		参数：n_estimators=10, 最终训练的子模型的数量
			criterion="gini", 决策树构建的时候"纯"度的衡量指标，默认gini，可选值为gini、entropy
			max_depth=None, 剪枝参数；用于限制决策树的深度
			min_samples_split=2, 剪枝参数；只有当数据集中的样本数目大于等于该值的时候，才进行数据的划分。
			min_samples_leaf=1,剪枝参数；要求分裂之后的子数据集中的样本数目至少为该值。
			max_samples=1.0, 每个子模型在训练的时候使用多少原始数据，给定的百分比，默认100%
			max_features='auto', 在随机森林中的每个决策树的每个划分节点的划分特征选择的时候，从多少个原始特征属性中获取最优特征。
			bootstrap=True, 对于每个子模型产生训练数据时候，是否做有放回的重采样，默认是
			oob_score=False, 是否计算袋外准确率(没有参与模型训练的数据称为oob数据<袋外数据>，其实就是当模型构建好之后，将oob数据输入到模型看一下效果)
	   
  2. Extra Tree
	   Extra Tree是RF的一个变种，原理基本和RF一样，区别如下：
	   1. RF会随机重采样来作为子决策树的训练集，而Extra Tree每个子决策树
	   采用原始数据集训练；
	   2. RF在选择划分特征点的时候会和传统决策树一样，会基于信息增益、
	   信息增益率、基尼系数、均方差等原则来选择最优特征值；而Extra Tree会
	   随机的选择一个特征值来划分决策树。
	   3. Extra Tree因为是随机选择特征值的划分点，这样会导致决策树的规模
	   一般大于RF所生成的决策树。也就是说Extra Tree模型的方差相对于RF进
	   一步减少。在某些情况下，Extra Tree的泛化能力比RF的强。
	   随机森林存在非常严重过拟合可考虑使用Extra Tree
  3. Totally Random Trees Embedding(TRTE) 数据维度扩展
		TRTE是一种非监督的数据转化方式。将低维的数据集映射到高维，
		从而让映射到高维的数据更好的应用于分类回归模型。
		无监督，RandomTreeEmbedding，没有predict(),有transform，用来做特征工程
  4. islation Forest   异常点检测
  
  5. apply 方法  ====》 方法返回其叶子节点下标
  

		
二、 Boosting（提升学习）：串行，数据一样,速度会慢
		boosting解决欠拟合问题，可以用于回归和分类。
  1. adaboost(Adaptive Boosting):改样本权重
	以错误率作为损失函数
    1.1 底层使用决策树，基学习器可以是其他类型。
	1.2 特点：
		 AdaBoost的优点如下：
		 可以处理连续值和离散值；
		 模型的鲁棒性比较强；
		 解释性强，结构简单。
		 AdaBoost的缺点如下：
		 对异常样本敏感，异常样本可能会在迭代过程中获得较高的权重值，最终影响模型效果。
		 学习率，限制子模型的能力，防止过拟合

  2. gbdt（Gradient Boosting）:改目标属性，底层：必须CART，都是回归树
		迭代决策树和随机森林的区别：
	随机森林使用抽取不同的样本构建不同的子树，也就是说第m棵树的构建和前m-1棵树的结果是没有关系的
	
	迭代决策树在构建子树的时候，使用之前子树构建结果后形成的残差作为输入数据构建下一个子树；然后最终预测的时候按照子树构建的顺序进行预测，并将预测结果相加
	
	GBDT的优点如下：
		 可以处理连续值和离散值；
		 在相对少的调参情况下，模型的预测效果也会不错；
		 模型的鲁棒性比较强。
	 GBDT的缺点如下：
		由于弱学习器之间存在关联关系，难以并行训练模型。
		也就是模型训练的速度慢。
	
	GBDT回归算法和分类算法的区别：
		两者唯一的区别就是选择不同的损失函数、以及对应的负梯度值和模型初值采用不一样的值
		回归算法选择的损失函数一般是均方差(最小二乘)【均值做为初始值】和绝对值误差【中值做为初始值】，分类算法中一般选择对数损失函数来表示【二分类以ln(正样本个数/负样本个数)作为初始值，多分类以0作为初值】
	
	apply(X)：array_like, shape = [n_samples, n_estimators, n_classes]【样本数*模型数*类别数】

	迭代决策树和随机森林的区别：
		•随机森林使用抽取不同的样本构建不同的子树，也就是说第m棵树的构建和前m-1棵树的结果是没有关系的
		•迭代决策树在构建子树的时候，使用之前子树构建结果后形成的残差作为输入数据构建下一个子树；然后最终预测的时候按照子树构建的顺序进行预测，并将预测结果相加

Bagging、Boosting的区别：
	•样本选择：Bagging算法是有放回的随机采样；Boosting算法是每一轮训练集不变，只是训练集中的每个样例在分类器中的权重发生变化或者目标属性y发生变化，而权重&y值都是根据上一轮的预测结果进行调整；
	•样例权重：Bagging使用随机抽样，样例是等权重；Boosting根据错误率不断的调整样例的权重值，错误率越大则权重越大(Adaboost)；
	•预测函数：Bagging所有预测模型的权重相等；Boosting算法对于误差小的分类器具有更大的权重（Adaboost）。
	•并行计算：Bagging算法可以并行生成各个基模型；Boosting理论上只能顺序生产，因为后一个模型需要前一个模型的结果；
	•Bagging是减少模型的variance(方差)；Boosting是减少模型的Bias(偏度)。
	•Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合；Boosting里每个分类模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合
	
		
   3. XGboost
	XGBoost是GBDT算法的一种变种，是一种常用的有监督集成学习算法；是一种伸缩性强、便捷的可并行构建模型的Gradient Boosting算法
	1. 底层决策树，支持CART、线性回归、逻辑回归
	
	XGBoost底层是决策树，但是跟决策树不一样了，采用的不是投票法和平均值法，w = -Gj/(Hj+λ)
	
	特点：支持并行
		列采样(column subsampling)：借鉴随机森林的做法，支持列抽样，不仅可以降低过拟合，还可以减少计算量；
		 支持对缺失值的自动处理。对于特征的值有缺失的样本，XGBoost可以自动学习分裂方向；
		 XGBoost支持并行。XGBoost的并行是特征粒度上的，在计算特征的Gain的时候，会并行执行，但是在树的构建过程中，还是串行构建的；
		 XGBoost算法中加入正则项，用于控制模型的复杂度，最终模型更加不容易过拟合；
		 XGBoost基学习器支持CART、线性回归、逻辑回归；
		 XGBoost支持自定义损失函数(要求损失函数二阶可导)。
	
	
	
	
	
	
	
	
	GBT参数：
		max_depth	给定树的深度，默认为3
		learning_rate	每个迭代产生的模型的权重/学习率，默认为0.1
		n_estimators	子模型的数量，默认为100
		objective	给定损失函数，默认为”binary:logistic”	给定损失函数，默认为”reg:linear”
		booster	给定模型的求解方式，默认为: gbtree；可选参数: gbtree、gblinear、dart
		n_jobs	使用多少个线程并行构建XGBoost模型，默认为1
		reg_alpha	L1正则项的权重，默认为0
		reg_lambda	L2正则项的权重，默认为1

  
  

  
  