特征工程：
	所有一切为了让模型效果变的更好的数据处理方式都可以认为属于特征工程这个范畴中的一个操作
	
	常规的特征工程需要处理的内容：
		• 异常数据的处理
		• 数据不平衡处理
		• 文本处理：词袋法、TF-IDF
		•多项式扩展、哑编码、标准化、归一化、区间缩放法、PCA、特征选择......
		•将均值、方差、协方差等信息作为特征属性，对特征属性进行对数转换、指数转换......
		• 结合业务衍生出一些新的特征属性....

	
	数据：一般给开发提需求，开发准备好放到数据库。从数据库下载来用。
	
数据清洗：
		
		
	数据不平衡问题：
	数据分布不均匀，空间上重叠，业务关注的是数量少的样本，希望把出现数量少的样本预测正确（高召回率）
		方式：
			1. 相差极大，直接异常点检测（oneclass）
			2. 更改类别权重
			3. 过（上）采样
			4. 欠（下）采样。
			5. 集成下采样: 多个模型共同决策
			6. 数据合成之SMOTE算法
			6. 了解，ENN、 RENN、 Tomek Link Removal
			
	
特征转换：
		指将原始数据中的字段数据进行转换操作，从而得到适合进行算法模型构建的输入数据(数值型数据)，在这个过程中主要包括但不限于以下几种数据的处理：
			• 文本数据转换为数值型数据
			• 缺省值填充
			• 定性特征属性哑编码
			• 定量特征属性二值化
			• 特征标准化与归一化
	
	原始数据类型：数值型、文本、图像（像素值）、音频

	文本数据：分词、转为词向量（词袋法、词集法、TF/IDF、Hash TF-IDF）
	
	
		feature_extraction.text.CountVectorizer		  词袋法
		feature_extraction.text.HashingVectorizer	 哈希
		feature_extraction.text.TfidfTransformer	
		feature_extraction.text.TfidfVectorizer	TF-IDF，相当于1+3
	
	
	缺省值处理：
		缺失率高重要性低：删除
		缺失率低重要性低：简单处理
		重要性高缺失率低：
			计算填充（均值、众数、中位数）、经验业务知识（专家）
		重要性高缺失率高：
			1. 尝试从其他渠道获取补全
			2. 使用其他字段计算（新模型）
			3. 去除字段（实在没法获取）
		
		
		查看影响重要性：查看x与y的协方差，为0不重要
		
		numpy做重要性的分析？？
		
	哑编码：
		常用于：回归算法(线性回归、Ridge、Lasso、Logistic回归等)、SVM(svc/svr)、KNN、KMeans等
		一般不需要哑编码的算法：决策树、贝叶斯、HMM等

	二值化/连续数据区间化：
		对于定量的数据（特征取值连续）根据给定的阈值，将其进行转换，如果大于阈值，那么赋值为1；否则赋值为0
		
		一般情况下，对于每个特征需要使用不同的阈值进行二值化操作
		一般情况下，对数据进行划分的时候，不是进行二值化，而是进行多值化(分区化/分桶化)；即：将一个连续的数据，按照不同的取值范围，分为不同的级别；比如：在某一个模型中，存在人的收入情况，单位为元，根据业务来判断的话，可能会得到影响因变量的因素其实是区间后的收入情况，那么这个时候就可以基于业务的特征，将收入划分为收入等级，比如：1w -> 0, 1w~2w -> 1, 2w~3w -> 2, 3w+ -> 3(至于这里到底做不做哑编码操作，具体的看对于区间化之后的数据的理解。如果认为区间化之后的特征数据是具有等级意义的，那么就可以考虑不做哑编码。如果认为区间化之后的数据是一个类别数据，那么一般需要做哑编码操作)
	
		一般二值化（区间化）后会进行哑编码操作
		
	
	标准化：StandardScalar
		基于特征属性的数据(也就是特征矩阵的列)，获取均值和标准差，然后将特征值转换至服从标准正态分布。
		x' = (x - X_aver) / S
		标准化的目的是为了降低不同特征的不同范围的取值对于模型训练的影响；比如对于同一个特征，不同的样本的取值可能会相差的非常大，那么这个时候一些异常小或者异常大的数据可能会误导模型的正确率；另外如果数据在不同特征上的取值范围相差很大，那么也有可能导致最终训练出来的模型偏向于取值范围大的特征，特别是在使用梯度下降求解的算法中；
		通过改变数据的分布特征，具有以下两个好处： 1.提高迭代求解的收敛速度；2. 提高迭代求解的精度/提高模型的效果
		
	
	区间缩放法：MinMaxScalar
		是指按照数据(特征属性，也就是列)的取值范围特性对数据进行等比例缩放操作，将数据缩放到给定区间上：x_std=(x-x.min)/(x.max-x.min)

	
	正则化：Normalizer对行数据转换
		从数据层面防止过拟合。
		和标准化不同，正则化是基于矩阵的行进行数据处理，其目的是将矩阵的行均转换为“单位向量”，
		l2规则转换公式如下：x'=x/sqrt(sum(x**2))
		l1规则转换公式如下：x'=x/sum(abs(x))
		max规则转换公式如下：x'=x/max(x)
	
		归一化对于不同特征维度的伸缩变换的主要目的是为了使得不同维度度量之间特征具有可比性，同时不改变原始数据的分布(相同特性的特征转换后，还是具有相同特性)(不改变的意思是：多个特征之间的关系不改变)。和标准化一样，也属于一种无量纲化的操作方式。
		正则化则是通过范数规则来约束特征属性，通过正则化我们可以降低数据训练处来的模型的过拟合可能，和之前在机器学习中所讲述的L1、L2正则的效果一样。在进行正则化操作的过程中，不会改变数据的分布情况，但是会改变数据特征之间的相关特性。
		备注：广义上来讲，标准化、区间缩放法、正则化都是具有类似的功能。在有一些书籍上，将标准化、区间缩放法统称为标准化，把正则化称为归一化操作。
		如果面试有人问标准化和归一化的区别：标准化会改变数据的分布情况，归一化不会，标准化的主要作用是提高迭代速度，降低不同维度之间影响权重不一致的问题
	
	**模型欠拟合，考虑标准化、归一化，过拟合考虑一下正则化。
	
	
	
特征选择：sklearn.feature_selection
	目的：
	1. 降低特征属性的数目，加快模型的训练迭代速度；
	2. 将一些影响小的、存在异常影响的特征属性删除掉，增加模型效果。
	太多的特征属性的存在可能会导致模型构建效率降低，同时模型的效果有可能会变的不好，那么这个时候就需要从所有特征属性中选择出影响最大的特征属性作为最后构建模型的特征属性列表
	在选择模型的过程中，通常从两方面来选择特征：
		•特征是否发散：如果一个特征不发散，比如方差等于0，也就是说这样的特征对于样本的区分没有什么作用。
		•特征与目标的相关性：如果与目标相关性比较高，应当优先选择。
		
	过滤法、包装法、嵌入法。
		过滤法：方差选择法、相关系数法、卡方检验、互信息法
			方差选择法：VarianceThreshold，分类、回归均可
			相关系数法：SelectKBest，使用评估指标：chi2，mutual_info_classif（分类），mutual_info_regression（回归），f_classif（分类），f_regression（回归）
			
			chi2，只可分类，特征值非负。
		
		包装法：
			递归特征消除法：RFE
		
		嵌入法：基于模型的特征选择
			SelectFromModel:基于惩罚项和树模型。
			
降维：

	
	主要目的：
		1. 降低特征属性的数目，加快模型的训练迭代速度；
		2. 将一些影响小的、存在异常影响的特征属性融合掉，增加模型效果。
		3. 将特征属性与特征属性转换为独立互不影响。
		4.将多个原始特征属性融合成为一个特征属性，这样在某些模型中，可以对于多个原始特征属性一起去衡量对于Y的影响

	主要方法：PCA（主成分分析法）、LDA（线性判别分析，仅分类）
	PCA是为了让映射后的样本具有更大的发散性(利用方差最大化理论)，PCA是无监督的学习算法；
	LDA是为了让映射后的样本有最好的分类性能，LDA是有监督学习算法。
	
	相同点：
		• 两者均可以对数据完成降维操作
		• 两者在降维时候均使用矩阵分解的思想
		• 两者都假设数据符合高斯分布
	 不同点：
		•LDA是监督降维算法，PCA是无监督降维算法
		•LDA降维的维度数目的取值范围为[1,n_class-1]，而PCA降维的维度数目的取值范围为[1,min(n_samples, n_features)]
		•LDA除了降维外，还可以应用于分类
		•LDA选择的是分类性能最好的投影，而PCA选择样本点投影具有最大方差的方向	

共线性问题：
	输入的自变量之间存在较高的线性相关度。共线性问题会导致回归模型的稳定生和准确性大大降低，过多无关的维度参与计算也会浪费计算资源和时间。
	(线性回归<最原始的线性回归>、Logisitc回归、SVM这些算法一般会受多重共线性问题的影响)
	
	解决方法：
		1. 降维
		2.正则化(相当于对于效果相同的特征属性，将部分特征属性的权重系数降低， 通过这种方式来缓解多重共线性的问题)	
		3.逐步回归迭代法：假设有10个特征属性，首先使用任意一个特征属性构建一个模型，得到当前模型的效果，然后任意加入一个新的特征属性，如果新加入的特征属性不能让模型效果变的更好，那么表示新加入的特征属性是无效特征属性，不需要考虑，如果新加入的特征属性让模型效果变好，那么表示新特征对于模板属性具有影响能力，所以考虑新加入的特征属性x2, 和老的特征属性x1,它们对应的权重系数θ2和θ1的变化情况，如果θ1变化很小，那么表示x1和x2之间是无关，如果θ1变化很大，那么表示x1和x2之间是有关的；最后迭代完后，将所有线性有关的特征属性以及对于目标属性没有决策能力的特征进行删除。(相当于将多重共线性的特征属性进行删除操作)

异常值的处理：


特征工程一般无特定顺序，如果要做特征选择和降维，先做特征选择再降维。