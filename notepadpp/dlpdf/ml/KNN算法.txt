KNN:
	近朱者赤，近墨者黑
	
即可以做分类，又可以做回归。
	KNN在做回归和分类的主要区别在于最后做预测的时候的决策方式不同。
		KNN在分类预测时，一般采用多数表决法；
		而在做回归预测时，一般采用平均值法

在KNN算法中，非常重要的主要是三个因素：
	K值的选择：K值选择过小导致过拟合，k值过大导致欠拟合。
	距离的度量：一般使用欧氏距离(欧几里得距离)；
	决策规则：在分类模型中，主要使用多数表决法或者加权多数表决法；在回归模型中，主要使用平均值法或者加权平均值法

KNN过拟合和欠拟合
  过拟合：
    产生原因：一般是由于K值选择过小导致。
  欠拟合：
    产生原因：一般是K值选择过大导致的。
	
knn分类：多数投票、加权的多数投票（距离小权重大）
knn回归：平均值法、加权平均值法


伪代码：
  简单来写：
    def fit(train, k):
	  self.train = train
	  self.k = k
	def predict(test):
	  # a. 从训练数据train中获取和当前数据test距离最近的k个样本
	  neighbors = fetch_k_neighbors(self.train, test, self.k)
	  # b. 合并这K个最近样本，得到预测值
	  predict_label = calc_predict_label(neighbors)
	  return predict_label
  
  复杂来写：
    def fit(train, k):
	  self.train = train
	  self.k = k
	def predict(test):
	  result = []
	  for x in test:
	    # a. 从训练数据train中获取和当前数据x距离最近的k个样本
	    neighbors = fetch_k_neighbors(self.train, x, self.k)
		
	    # b. 合并这K个最近样本，得到预测值
		# b1. 统计一下各个类别label出现的次数
		label_2_count_dict = {}
		for neighbor in neighbors:
		  # b11. 获取当前样本neighbor的标签值
		  label = neighbor.label
		  # b12. 将这个label添加到字典中
		  if label not in label_2_count_dict:
		    label_2_count_dict[label] = 1
		  else:
		    label_2_count_dict[label] += 1
		# b2. 从这个字典中获取出现次数最多的label标签值作为预测值
		max_label_count = 0
		max_label = None
		for label in label_2_count_dict:
	      # 获取当前label对应出现的count数量
		  count = label_2_count_dict[label]
		  # 将当前count和最大值进行比较，选择/保留最大的count
		  if count > max_label_count:
		    max_label_count = count
			max_label = label
		# b3. 将预测值添加到集合中
		result.append(max_label)
	  return result

直接计算距离计算量大，使用KDTree算法求解最近样本。

-1.KNN
		-a. 执行原理/执行过程:
			利用“近朱者赤，近墨者黑”的思想，算法认为如果在样本空间上，两个样本具有相似的特征属性，那么可以认为这两个样本具有相同/相近的目标属性。
			执行步骤:(预测的过程)
			 --1. 获取带预测样本的K个最相似的邻近样本
			 --2. 将这K个邻近样本的目标属性做一个融合，得到预测值。
		-b. KNN的伪代码：
		-c. KNN的问题以及优化方式：
			--1. 距离对于样本的影响
			--2. 加权的思想的影响
			--3. K值的影响
			--4. 当训练样本比较多的时候，直接获取K个邻近样本的操作是比较耗时的，性能效率不高 ---> KDTree

