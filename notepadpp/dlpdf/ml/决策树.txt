决策树：类似于if else判断

信息熵的概念：
	H(X)=-sum(p*log(p))
	条件熵：H(Y|X) = H(X,Y)-H(X)
	
决策树分为两大类：
	分类树和回归树，前者用于分类标签值，后者用于预测连续值，
	常用算法有ID3、C4.5、CART等

根据特征属性的类型不同，在构建决策树的时候，采用不同的方式，具体如下：
	属性是离散值，而且不要求生成的是二叉决策树，此时一个属性就是一个分支
	属性是离散值，而且要求生成的是二叉决策树，此时使用属性划分的子集进行测试，按照“属于此子集”和“不属于此子集”分成两个分支
	属性是连续值，可以确定一个值作为分裂点split_point，按照>split_point和<=split_point生成两个分支

NOTE：决策树中会将区分能力比较强的特征属性放到决策树的根节点位置进行数据的划分操作。(((可以用于特征选择)))

决策树的构建是基于样本概率和纯度进行构建操作的，那么进行判断数据集是否“纯”可以通过三个公式进行判断，分别是Gini系数、熵(Entropy)、错误率
	Gini系数：1-sum(p**2)
	Entropy: -sum(p*log(p))
	Error: 1-max(p)
	
信息增益：Gain = H(D) - H(D|A)


ID3算法是决策树的一个经典的构造算法，内部使用信息熵以及信息增益来进行构建；每次迭代选择信息增益最大的特征属性作为分割属性
 1. ID3算法只支持离散的特征属性，不支持连续的特征属性
 2. ID3算法构建的是多叉树

ID3算法:
优点:
	决策树构建速度快；实现简单；
缺点：
	计算依赖于特征取值数目较多的特征，而属性值最多的属性并不一定最优
	ID3算法不是递增算法
	ID3算法是单变量决策树，对于特征属性之间的关系不会考虑
	抗噪性差
	只适合小规模数据集，需要将数据放到内存中

C4.5算法
使用信息增益率来取代ID3算法中的信息增益，在树的构造过程中会进行剪枝操作进行优化；能够自动完成对连续属性的离散化处理；C4.5构建的是多分支的决策树；
优点：
	产生的规则易于理解
	准确率较高
实现简单
缺点：
	对数据集需要进行多次顺序扫描和排序，所以效率较低
	只适合小规模数据集，需要将数据放到内存中


CART算法
使用基尼系数(分类树)作为数据纯度的量化指标来构建的决策树算法就叫做CART(Classification And Regression Tree，分类回归树)算法。CART算法使用GINI增益率作为分割属性选择的标准，选择GINI增益率最大的作为当前数据集的分割属性；可用于分类和回归两类问题。强调备注：CART构建是二叉树。


ID3、C4.5、CART分类树算法总结:
ID3、C4.5和CART算法均只适合在小规模数据集上使用（数据集必须可以完全加载到内存中）
ID3、C4.5和CART算法都是单变量决策树
当属性值取值比较多的时候，最好考虑C4.5算法，ID3得出的效果会比较差
决策树分类一般情况只适合小数据量的情况(数据可以放内存)
CART算法是三种算法中最常用的一种决策树构建算法(sklearn中仅支持CART)。
三种算法的区别仅仅只是对于当前树的评价标准不同而已，ID3使用信息增益、C4.5使用信息增益率、CART使用基尼系数。（不是主要区别）
CART算法构建的一定是二叉树，ID3和C4.5构建的不一定是二叉树。(主要区别)


算法	支持模型	树结构	划分特征选择	连续值处理	缺失值处理	剪枝	特征属性多次使用
ID3	分类	多叉树	信息增益	不支持	不支持	不支持	不支持
C4.5	分类	多叉树	信息增益率	支持	 支持	支持	不支持
CART	分类、回归	二叉树	基尼系数、均方差	支持	支持	支持	支持


在回归树中，叶子节点的预测值一般为叶子节点中所有值的均值来作为当前叶子节点的预测值。所以在回归树中一般采用MSE或者MAE作为树的评价指标，即均方差。

sklearn中仅支持CART,重要的参数是max_depth，重要的属性是feature_importance

决策树中树的复杂程度会影响决策树的拟合效果
  过拟合：
    一般情况由于决策树的深度太深导致
	解决方案：
	  剪枝(限制决策树的深度)
	  集成学习->Bagging解决(随机森林)
  欠拟合:
    一般情况由于决策树的深度太浅导致
	解决方案：
	  增加深度
	  集成学习-Boosting解决(GBDT)

======================================
-2.决策树
		-a. 执行原理/执行过程:
			思想：决策树是一种基于历史数据的概率关系进行数据划分的一种算法。在对数据进行划分的时候，希望将数据尽可能的划分的足够的“纯”，也就是最好是划分之后的数据集中只有一个类别的数据。
		-b. 过拟合和欠拟合
			决策树的模型复杂的的设置
			过拟合：
				剪枝
				随机森林
			欠拟合：
				增加树的复杂程度(深度)
				AdaBoost集成学习
			



