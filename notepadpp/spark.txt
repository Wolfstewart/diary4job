Spark 
	
	
	
	DataFrame:
		https://www.jianshu.com/p/acd96549ee15
		
		https://zhuanlan.zhihu.com/p/102014548



sparkstreaming submit:

spark-submit --master local[3] --driver-memory 5g --num-executors 5 --executor-cores 1 --executor-memory 1G  --conf spark.pyspark.python=/usr/local/python3/bin/python3.6 --conf spark.pyspark.driver.python=/usr/local/python3/bin/python3.6 cloud_spark_kafka_test_from_mongo.py


spark的调度模式：
	https://www.cnblogs.com/lenmom/p/11285273.html
	https://www.cnblogs.com/francisYoung/p/5209798.html
	https://blog.csdn.net/qq_34382453/article/details/83615000
	
	
	
spark为 DataFrame join指定多个列条件:
	https://www.runexception.com/q/836
	
	如何在 join两个 DataFrame 时提供更多的列条件。例如，我想运行以下内容：

	Leaddetails.join(
    Utm_Master, 
    Leaddetails("LeadSource") <=> Utm_Master("LeadSource")
        && Leaddetails("Utm_Source") <=> Utm_Master("Utm_Source")
        && Leaddetails("Utm_Medium") <=> Utm_Master("Utm_Medium")
        && Leaddetails("Utm_Campaign") <=> Utm_Master("Utm_Campaign"),
    "left"
	)
	
	
	List Seq 可以隐式转换生成DataFrame
	
	需要将influxRecord 数组转为DataFrame，


流处理保存小文件问题：
	spark流式计算如果处理小文件，sparkstreaming , hdfs + coalese, 追加不支持压缩，delta的snappy不行。
	https://www.zhihu.com/zvideo/1300201847928647680
	
	
	合并：https://blog.csdn.net/weixin_39534395/article/details/111577388
	
	
delta lake:
	https://docs.delta.io/latest/quick-start.html#set-up-apache-spark-with-delta-lake&language-scala
	
	
spark 行差:
	https://blog.csdn.net/shinever1/article/details/99863804
	
Trigger:
	.trigger(Trigger.Once())		, 只执行一次，遗留数据处理的场景
	.trigger(Trigger.ProcessingTime("2 seconds"))
	.trigger(Trigger.Continuous("1 second"))	连续流处理，一条条处理，可以设置毫秒级
	只支持 Map 类的有类型操作
	只支持普通的的 SQL 类操作, 不支持聚合
	Source 只支持 Kafka
	Sink 只支持 Kafka, Console, Memory

Spark DataFrame 用户自定义（聚合）函数：
	https://www.jianshu.com/p/ddc39b4f2bdf
		
	